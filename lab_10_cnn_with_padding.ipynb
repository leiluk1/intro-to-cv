{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZ9_2xFUzdU"
      },
      "source": [
        "# Introduction to Computer Vision - Lab 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9DCQAaFFQyw"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Incorporate padding to your convolutional neural network you are developing this week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LFJ23Sa5U-pa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(0)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q11tzNkIlLc",
        "outputId": "f21f6214-3d2c-4e52-8da7-c72cd0b3acaa"
      },
      "outputs": [],
      "source": [
        "# load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pEp_yT72IlLd"
      },
      "outputs": [],
      "source": [
        "# decrease the size of the dataset because of the computational cost\n",
        "x_train, y_train, x_test, y_test = x_train[:5000], y_train[:5000], x_test[:1000], y_test[:1000]\n",
        "\n",
        "# shuffle the train set images\n",
        "indices = np.arange(len(x_train))\n",
        "np.random.shuffle(indices)\n",
        "x_train, y_train = x_train[indices], y_train[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "VuFuPz24IlLd",
        "outputId": "e3e393ce-ce40-4ab4-de8e-21fb811d3bcb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the first images from training set with the corresponding labels\n",
        "fig, axs = plt.subplots(nrows=1, ncols=4)\n",
        "\n",
        "for i in range(4):\n",
        "    ax = axs[i]\n",
        "    ax.imshow(x_train[i], cmap='gray')\n",
        "    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    ax.set_title(y_train[i],\n",
        "                 fontsize=9,\n",
        "                 y=-0.15)\n",
        "\n",
        "plt.suptitle('Train images from MNIST datasat',\n",
        "             y=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdhOunQUIlLe",
        "outputId": "1a84d8ea-c6a7-4709-d2ba-1a069bd3ec49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reshape the images \n",
        "x_train = x_train.reshape(-1, 1, 28, 28)\n",
        "x_test = x_test.reshape(-1, 1, 28, 28)\n",
        "x_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IREwWbGlIlLe",
        "outputId": "1f3022df-d138-4553-9d9e-2957548fc3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "(10,)\n"
          ]
        }
      ],
      "source": [
        "# represent each label as a true label vector with zeros and 1 on the label position\n",
        "def vectorize_y(y):\n",
        "    y_vec = np.zeros((len(y), 10)) # since there are 10 possible labels\n",
        "    for i in range(len(y)):\n",
        "        y_vec[i, y[i]] = 1\n",
        "    return y_vec\n",
        "\n",
        "\n",
        "# vectorize\n",
        "y_train_vec = vectorize_y(y_train)\n",
        "print(y_train_vec[0])\n",
        "print(y_train_vec[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convolutional Layer with padding added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "32-yd_muVffw"
      },
      "outputs": [],
      "source": [
        "# Class representing the Convolutional Node with padding added\n",
        "class ConvNode:\n",
        "\n",
        "    def __init__(self, padding):\n",
        "        self.Zk_prev = None # the previous feature map\n",
        "        self.Wk = None # the filter\n",
        "        self.padding = padding # padding size\n",
        "\n",
        "    def forward(self, Zk_prev, Wk):\n",
        "        # Forward propagation\n",
        "        self.Zk_prev = Zk_prev\n",
        "        self.Wk = Wk\n",
        "\n",
        "        # Apply convolution operation\n",
        "        depth, h, w = self.Zk_prev.shape\n",
        "        _, hw, ww = self.Wk.shape\n",
        "        \n",
        "        # Apply padding to the input feature map\n",
        "        self.Zk_prev = np.pad(self.Zk_prev, ((0, 0), \n",
        "                                             (self.padding, self.padding), \n",
        "                                             (self.padding, self.padding)), \n",
        "                              mode='constant')\n",
        "        \n",
        "        Zk_h = h + 2 * self.padding - hw + 1\n",
        "        Zk_w = w + 2 * self.padding - ww + 1\n",
        "\n",
        "        Zk = np.zeros((Zk_h, Zk_w))\n",
        "\n",
        "        for d in range(depth):\n",
        "            Zk += self.convolve(self.Zk_prev[d, :, :], self.Wk[d, :, :])\n",
        "\n",
        "        return Zk\n",
        "\n",
        "    def backward(self, dL_dZk):\n",
        "        # Bacward propagation\n",
        "        dL_dWk = np.zeros_like(self.Wk)\n",
        "        dL_dZk_prev = np.zeros_like(self.Zk_prev)\n",
        "\n",
        "        # Compute gradients using formulas (5) and (9) from lecture slides\n",
        "        for d_i in range(self.Wk.shape[0]):\n",
        "            dL_dWk[d_i, :, :] = self.convolve(self.Zk_prev[d_i, :, :], dL_dZk)\n",
        "            dL_dZk_prev[d_i, :, :] = self.convolve(self.pad(dL_dZk, self.Wk),\n",
        "                                                   self.rotate(self.Wk[d_i, :, :]))\n",
        "        \n",
        "        # Remove padding from gradients\n",
        "        dL_dZk_prev = dL_dZk_prev[:, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        dL_dWk = dL_dWk[:, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        \n",
        "        return dL_dWk, dL_dZk_prev\n",
        "\n",
        "    def convolve(self, X, W):\n",
        "        # Convolution operation between X and W that produces the matrix Z\n",
        "        h, w = X.shape\n",
        "        hw, ww = W.shape\n",
        "\n",
        "        Z_h = h - hw + 1\n",
        "        Z_w = w - ww + 1\n",
        "        Z = np.zeros((Z_h, Z_w))\n",
        "\n",
        "        for i in range(Z_h):\n",
        "            for j in range(Z_w):\n",
        "                Z[i, j] = np.sum(X[i:i+hw, j:j+ww] * W)\n",
        "\n",
        "        return Z\n",
        "\n",
        "    def rotate(self, Z):\n",
        "        # Rotate the matrix Z by 180 degrees\n",
        "        return np.flip(np.flip(Z, 1), 0)\n",
        "\n",
        "    def pad(self, Z, Wk):\n",
        "        # Pad the matrix Z with zeros\n",
        "        return np.pad(Z, ((Wk.shape[1]-1, Wk.shape[1]-1),\n",
        "                          (Wk.shape[2]-1, Wk.shape[2]-1)),\n",
        "                        mode='constant')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZsHeCj7L9DZT"
      },
      "outputs": [],
      "source": [
        "# Class representing the Convolutional Layer\n",
        "class ConvLayer:\n",
        "\n",
        "    def __init__(self, D, filter_shape, Bk, padding=0):\n",
        "        # D - number of filters\n",
        "        self.D = D\n",
        "        self.padding = padding\n",
        "        # initialize the D filters\n",
        "        self.filters = np.array([np.random.randn(*filter_shape) / 9 for _ in range(D)])\n",
        "        self.conv_nodes = [ConvNode(padding) for _ in range(D)]\n",
        "        self.Bk = Bk\n",
        "        self.Zk = np.zeros_like(Bk)\n",
        "\n",
        "    def forward(self, Zk_prev):\n",
        "        # forward propagation\n",
        "        self.Zk_prev = Zk_prev\n",
        "        self.Zk = np.zeros_like(self.Bk)\n",
        "\n",
        "        # apply convolution operation for each filter and after add bias\n",
        "        for d in range(self.D):\n",
        "            Zk_tilde = self.conv_nodes[d].forward(self.Zk_prev, self.filters[d])\n",
        "            self.Zk[d, :, :] = Zk_tilde + self.Bk[d, :, :]\n",
        "\n",
        "        return self.Zk\n",
        "\n",
        "    def backward(self, dL_dZk):\n",
        "        # backward propagation\n",
        "        dL_dBk = dL_dZk\n",
        "        \n",
        "        dL_dWks = np.zeros_like(self.filters)\n",
        "        dL_dZk_prev = np.zeros_like(self.Zk_prev)\n",
        "\n",
        "        # compute gradients using formulas from lecture slides\n",
        "        for d in range(self.D):\n",
        "            dL_dWk, dL_dZk_prev_d = self.conv_nodes[d].backward(dL_dZk[d, :, :])\n",
        "            dL_dWks[d] = dL_dWk\n",
        "            dL_dZk_prev += dL_dZk_prev_d\n",
        "\n",
        "        return dL_dBk, dL_dWks, dL_dZk_prev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BiQhCRBoGjmu"
      },
      "outputs": [],
      "source": [
        "# Class representing the ReLU node (from the previous labs)\n",
        "class ReLU:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.x = None  # input values\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculate output of relu function\n",
        "        self.x = x\n",
        "        return np.maximum(0, self.x)\n",
        "\n",
        "    def backward(self, loss_grad):\n",
        "        # compute gradients with respect to the input values\n",
        "        relu_der = np.where(self.x < 0, 0, 1) # local gradient of relu\n",
        "        grad = relu_der * loss_grad\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hzea_VMFGVG8"
      },
      "outputs": [],
      "source": [
        "# Class representing the Softmax node (from the previous labs)\n",
        "class SoftmaxNode:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.y_hat = None  # output of forward propagation\n",
        "        self.grad = None  # gradient with respect to the input x (result of backward propagation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculate output of the softmax function with normalization\n",
        "        self.x = x\n",
        "        self.x_norm = self.x / np.max(np.abs(self.x))\n",
        "        e_x = np.exp(self.x_norm)\n",
        "        self.y_hat = e_x / np.sum(e_x)\n",
        "        return self.y_hat\n",
        "\n",
        "    def backward(self, loss_grad):\n",
        "        # compute gradients\n",
        "        self.y_hat = self.y_hat.reshape(-1)\n",
        "\n",
        "        softmax_grad = loss_grad * self.y_hat * (1 - self.y_hat)  # derivative of softmax\n",
        "\n",
        "        softmax_grad = softmax_grad.reshape(-1)\n",
        "        self.x = self.x.reshape(-1)\n",
        "\n",
        "        grad = np.dot(self.x, softmax_grad) / (-np.max(np.abs(self.x))**2)\n",
        "\n",
        "        self.grad = np.zeros_like(self.x, dtype=float)\n",
        "        self.grad[np.argmax(self.x)] = grad\n",
        "        self.grad *= np.sign(self.x)\n",
        "        self.grad += softmax_grad * (1 / np.max(np.abs(self.x)))\n",
        "\n",
        "        if np.max(np.abs(self.x)) < 1e-6:\n",
        "            raise Exception(\"biba\")\n",
        "\n",
        "        return self.grad.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorization Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5Ja2NxjhHZVN"
      },
      "outputs": [],
      "source": [
        "# Class representing the Vectorization Layer\n",
        "class VectorizationLayer:\n",
        "    def __init__(self):\n",
        "        self.input_shape = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input_shape = X.shape\n",
        "        return X.reshape(-1, 1)\n",
        "\n",
        "    def backward(self, dL_dZ):\n",
        "        return dL_dZ.reshape(self.input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fully-Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J33dqkHKIlLf"
      },
      "outputs": [],
      "source": [
        "# Matrix multiplication node (from the previous labs)\n",
        "class MulNode:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.w = None  # input matrix W\n",
        "        self.x = None  # input values x\n",
        "        self.grad = None  # gradient with respect to W (result of backward propagation)\n",
        "\n",
        "    def forward(self, w, x):\n",
        "        # calculate output of multiplication W*X\n",
        "        self.w, self.x = w, x\n",
        "        return np.matmul(self.w, self.x)\n",
        "\n",
        "    def backward(self, dL_dz):\n",
        "        # compute gradient with respect to W\n",
        "        dz_dw = self.x.reshape((1, self.x.shape[0]))\n",
        "        self.grad = np.matmul(dL_dz, dz_dw)\n",
        "        return self.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wZ5gJrr5IlLg"
      },
      "outputs": [],
      "source": [
        "# Class representing the Fully Connected Layer\n",
        "class FullyConnectedLayer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.mult = MulNode()\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, X, W, b):\n",
        "        # forward propagation\n",
        "        self.W = W\n",
        "        Z = self.mult.forward(self.W, X)\n",
        "        return Z + b\n",
        "\n",
        "    def backward(self, dL_dZ):\n",
        "        # backward propagation\n",
        "        dL_db = dL_dZ\n",
        "        dL_dW = self.mult.backward(dL_dZ)\n",
        "        dL_dX = np.dot(np.transpose(self.W), dL_dZ)\n",
        "        return dL_db, dL_dW, dL_dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oawhzD6dIlLg"
      },
      "outputs": [],
      "source": [
        "# Class representing the Convolutional Neural Network from the lecture slide 22\n",
        "class CNN:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.b1 = np.zeros((1, 28, 28))\n",
        "        self.b2 = np.zeros((1, 28, 28))\n",
        "        self.b3 = np.zeros((10, 1))\n",
        "        self.W3 = np.random.randn(10, 28*28) / 10\n",
        "        self.conv_layer1 = ConvLayer(D=1, filter_shape=(1, 5, 5), Bk=self.b1, padding=2)\n",
        "        self.relu1 = ReLU()\n",
        "        self.conv_layer2 = ConvLayer(D=1, filter_shape=(1, 3, 3), Bk=self.b2, padding=1)\n",
        "        self.relu2 = ReLU()\n",
        "        self.vectorize = VectorizationLayer()\n",
        "        self.fc = FullyConnectedLayer()\n",
        "        self.softmax = SoftmaxNode()\n",
        "\n",
        "    def forward(self, X):\n",
        "        # forward propagation\n",
        "        Z = self.conv_layer1.forward(X)\n",
        "        Z = self.relu1.forward(Z)\n",
        "        Z = self.conv_layer2.forward(Z)\n",
        "        Z = self.relu2.forward(Z)\n",
        "        Z = self.vectorize.forward(Z)\n",
        "        Z = self.fc.forward(Z, self.W3, self.b3)\n",
        "        Z = self.softmax.forward(Z)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dL_dZ):\n",
        "        # backward propagation\n",
        "        dL_dz = self.softmax.backward(dL_dZ)\n",
        "        dL_db3, dL_dW3, dL_dz = self.fc.backward(dL_dz)\n",
        "        dL_dz = self.vectorize.backward(dL_dz)\n",
        "        dL_dz = self.relu2.backward(dL_dz)\n",
        "        dL_dB2, dL_dW2, dL_dz = self.conv_layer2.backward(dL_dz)\n",
        "        dL_dz = self.relu1.backward(dL_dz)\n",
        "        dL_dB1, dL_dW1, dL_dZ = self.conv_layer1.backward(dL_dz)\n",
        "        return dL_dB1, dL_dW1[0], dL_dB2, dL_dW2[0], dL_db3, dL_dW3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbuBvvSGIlLg",
        "outputId": "f90fe7e3-e0b4-4e35-cf8b-150ae2196bd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.03620474],\n",
              "       [0.13700877],\n",
              "       [0.05878997],\n",
              "       [0.06798458],\n",
              "       [0.16636435],\n",
              "       [0.12497993],\n",
              "       [0.07763643],\n",
              "       [0.16879028],\n",
              "       [0.03930202],\n",
              "       [0.12293893]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the forward propagation for the first image in the train set\n",
        "cnn = CNN()\n",
        "cnn.forward(x_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcX8hw92IlLg",
        "outputId": "20edf461-dfd1-416c-8bdc-7183d4e1990c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1, 28, 28), (1, 5, 5), (1, 28, 28), (1, 3, 3), (10, 1), (10, 784))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the shapes of output of the backward propagation\n",
        "dL_dB1, dL_dW11, dL_dB2, dL_dW2, dL_db3, dL_dW3 = cnn.backward(np.ones((1, 10)))\n",
        "dL_dB1.shape, dL_dW11.shape, dL_dB2.shape, dL_dW2.shape, dL_db3.shape, dL_dW3.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform the Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jlSYX5qIlLh",
        "outputId": "d422221d-ef1a-4053-843a-13134766bdd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:46<00:00, 40.68s/it]\n"
          ]
        }
      ],
      "source": [
        "# define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# define the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# define the learning rate\n",
        "eta = 1.0\n",
        "\n",
        "# define the number of batches\n",
        "num_batches = len(x_train) // batch_size\n",
        "\n",
        "# perform the Mini-Batch Gradient Descent\n",
        "\n",
        "# loop over the epochs\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "\n",
        "    # initialize the gradients\n",
        "    dL_db1 = np.zeros_like(cnn.b1)\n",
        "    dL_db2 = np.zeros_like(cnn.b2)\n",
        "    dL_db3 = np.zeros_like(cnn.b3)\n",
        "    dL_dW1 = np.zeros_like(cnn.conv_layer1.filters[0])\n",
        "    dL_dW2 = np.zeros_like(cnn.conv_layer2.filters[0])\n",
        "    dL_dW3 = np.zeros_like(cnn.W3)\n",
        "\n",
        "    # loop over the batches\n",
        "    for batch in range(1, num_batches + 1):\n",
        "        # get the batch data\n",
        "        batch_x = x_train[(batch-1)*batch_size:batch*batch_size]\n",
        "        batch_y = y_train_vec[(batch-1)*batch_size:batch*batch_size]\n",
        "        \n",
        "        # loop over the batch data\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "\n",
        "            # perform forward propagation\n",
        "            y_hat = cnn.forward(x).reshape(-1)\n",
        "            \n",
        "            # compute the loss\n",
        "            l = -np.sum(y * np.log(y_hat))\n",
        "\n",
        "            # compute the gradient of the loss \n",
        "            grad_l = -y / y_hat\n",
        "\n",
        "            # compute the gradients\n",
        "            dL_db1_i, dL_dW1_i, dL_db2_i, dL_dW2_i, dL_db3_i, dL_dW3_i = cnn.backward(grad_l)\n",
        "            dL_db1 += dL_db1_i\n",
        "            dL_db2 += dL_db2_i\n",
        "            dL_db3 += dL_db3_i\n",
        "            dL_dW1 += dL_dW1_i\n",
        "            dL_dW2 += dL_dW2_i\n",
        "            dL_dW3 += dL_dW3_i\n",
        "\n",
        "\n",
        "       # update the weights and biases\n",
        "        cnn.b1 -= eta * dL_db1 / batch_size\n",
        "        cnn.conv_layer1.filters[0] -= eta * dL_dW1 / batch_size\n",
        "        cnn.b2 -= eta * dL_db2 / batch_size\n",
        "        cnn.conv_layer2.filters[0] -= eta * dL_dW2 / batch_size\n",
        "        cnn.b3 -= eta * dL_db3 / batch_size\n",
        "        cnn.W3 -= eta * dL_dW3 / batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmYPgM_TIlLh",
        "outputId": "2decea5b-2df9-4984-9b4d-1058155a2cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7510\n"
          ]
        }
      ],
      "source": [
        "# evaluate the model on the test set\n",
        "y_pred = []\n",
        "\n",
        "# loop over test images\n",
        "for x in x_test:\n",
        "    # perform forward propagation to get the predicted label\n",
        "    y_hat = cnn.forward(x)\n",
        "    y_pred.append(np.argmax(y_hat))\n",
        "\n",
        "# calculate the accuracy\n",
        "accuracy = sum(y_pred == y_test) / len(y_test)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The obtained accuracy of the CNN model (with architecture from lecture slides and padding added) on MNIST test dataset is $75.1$%. This indicates an improvement by approximately $2$%, comparing with the result of the previous lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6N9t9PMVWjY"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Code a function that takes as inputs the size of the input image and the desired size of the output image size at the k-th CONV-layer, and produces as output the appropriate padding and filter sizes, such that the amount of padding is minimized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "77AFD5sAMU5F"
      },
      "outputs": [],
      "source": [
        "def get_pad_and_filter_size(input_size, output_size):\n",
        "    h_in, w_in = input_size\n",
        "    h_out, w_out = output_size\n",
        "    \n",
        "    # the size of the filter W and size of the padding if no padding is applied\n",
        "    W_size = [h_in - h_out + 1, w_in - w_out + 1]\n",
        "    pad_size = [0, 0]\n",
        "    \n",
        "    \n",
        "    # if the filter W size is less than 1 in either dimension, needed to add padding\n",
        "    if W_size[0] < 1:\n",
        "        size_dif = 1 - W_size[0]\n",
        "        if size_dif % 2 == 0:\n",
        "            pad_size[0] = size_dif // 2 \n",
        "            W_size[0] += size_dif\n",
        "            \n",
        "        else:\n",
        "            pad_size[0] = (size_dif + 1) // 2 \n",
        "            W_size[0] += size_dif + 1\n",
        "    \n",
        "    # if the filter W size is less than 1 in either dimension, needed to add padding     \n",
        "    if W_size[1] < 1:\n",
        "        size_dif = 1 - W_size[1]\n",
        "        if size_dif % 2 == 0:\n",
        "            pad_size[1] = size_dif // 2 \n",
        "            W_size[1] += size_dif\n",
        "            \n",
        "        else:\n",
        "            pad_size[1] = (size_dif + 1) // 2 \n",
        "            W_size[1] += size_dif + 1\n",
        "        \n",
        "    \n",
        "    return pad_size, W_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EUj7m6HDMYG8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padding size for the convolutional layer: [1, 1]\n",
            "Filter size for the convolutional layer: [2, 2]\n"
          ]
        }
      ],
      "source": [
        "# some example\n",
        "print('Padding size for the convolutional layer:', get_pad_and_filter_size(input_size=[28, 28], output_size=[29, 29])[0])\n",
        "print('Filter size for the convolutional layer:', get_pad_and_filter_size(input_size=[28, 28], output_size=[29, 29])[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padding size for the convolutional layer: [0, 0]\n",
            "Filter size for the convolutional layer: [7, 7]\n"
          ]
        }
      ],
      "source": [
        "# some example\n",
        "print('Padding size for the convolutional layer:', get_pad_and_filter_size(input_size=[28, 28], output_size=[22, 22])[0])\n",
        "print('Filter size for the convolutional layer:', get_pad_and_filter_size(input_size=[28, 28], output_size=[22, 22])[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
